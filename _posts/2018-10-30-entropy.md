---
title: information entropy
date: 2018-10-30 19:30:00 +0900
categories: [machine learning]
tags: [information theory, entropy, machine learning]
---
### Information Entropy (Shannon Entropy)
사건 X의 불확실성 또는 정보량의 기대 값
- 사건 X는 K가지 경우의 수를 가진 이산 변수. 예를들어 6가지 경우가 나오는 주사위 던지기

모든 사건의 기대값 = Σ 확률변수 * 확률 
- 주사위 던지기의 기대값 = 1*1/6 + 2*1/6 + 3*1/6 + 4*1/6 + 5*1/6 + 6*1/6 = 3.5 

모든 사건 정보량의 기대값 
- H(P) = -Σ log(P(x)) * P(x)

각 사건의 발생 확률이 고르다면, 이 사건은 불확실성이 높다. 
- 주사위 1~6 눈이 나올 확률이 고르다면, 주사위를 던졌을 때 누가 나올지 불확실하다. 

각 사건의 발생 확률이 편중되어있다면, 이 사건은 불확실정이 낮다. 
- 무조건 앞면이 나오는 동전이라면, 동전을 던졌을 때 누가 나올지 확실하다. 

계산의 예 
- 고른 경우: -((1/2)*ln(1/2)+(1/2)*ln(1/2)) = 0.69 
- 편중된 경우: -((9/10)*ln(9/10)+(1/10)*ln(1/10)) = 0.32 

