[ { "title": "회의를 잘 하기 위한 준비물", "url": "/posts/how-to-make-meeting-efficient/", "categories": "dev, project mgmt", "tags": "pm, meeting", "date": "2021-01-14 00:54:00 +0900", "snippet": "작년에 제안해서 준비해오던 프로젝트가 드디어 조금씩 진행되고 있다. 프로젝트의 방향성을 정하고 만들 data product의 세부적인 방향, 기준에 대해 정의하기 위해 10여명이 모여 4시간에 이르는 회의를 했다. 주어진 4시간 안에 원하는 결과를 도출하기 위해서는 회의를 효율적으로 의도대로 진행하는 것이 중요해보였다. “그냥 다 같이 이야기나 한 번 해보시죠”하고 준비없이 회의에 들어갔다가는 10명의 4시간. 그러니까 40시간을 통채로 날려먹을 수도 있기 때문이다.그러자면 일단 회의에 들어오기 전에 참석자들이 이 회의가 무슨..." }, { "title": "100일 동안 매일 TIL commit 하기 회고", "url": "/retrospective/100TIL/", "categories": "dev, retrospective", "tags": "TIL", "date": "2019-01-22 18:50:00 +0900", "snippet": "100일 동안 매일 TIL commit 하기 (시즌2)가 오늘로 끝난다.시즌 1의 저조했던 출석률에 비하면 이번에는 50% 넘게 성공했으니,2일에 한 번 꼴로는 무언가 그낭 공부한 것을 요약하여 git에 올린 셈이다.이번 시즌에는 주로 “하이 퍼포먼스 스파크” 스터디 한 내용과 “Tour Of Scala” 예제 따라해본 것을 위주로 올렸다.초반 12일 동안은 매일 매일 꾸준히 했고, 이후 주말에 빠지기 시작하면서 결석일이 생겨났다.41일차부터 4-5일 연속으로 빠지는 일이 생기면서 출석률이 많이 떨어지기 시작했다.아무래도 나는..." }, { "title": "principles for the unit test of data processing job", "url": "/posts/spark-job-unit-test/", "categories": "dev, spark", "tags": "spark, unit test", "date": "2019-01-19 12:50:00 +0900", "snippet": "spark job 개발하다가 몇 가지 회고 결과물을 빨리 내겠다고 마음이 급해지니 제일 먼저 UnitTest 개발을 Skip 한다. 여기서부터 코드에서 나쁜 냄새가 나기 시작한다. 가능한한 연산의 과정, 과정을 method로 나누고 UnitTest를 만들자. 특히 numpy나 pandas로 matrix 연산을 하고 있다면, 그리고 vector space가 크다면, 더더욱 그렇게 해야 한다. 안그러면 연산 결과에 대한 검증을 하기가 힘들다. 단, ..." }, { "title": "함께 자라기", "url": "/posts/road-to-agile/", "categories": "book", "tags": "agile", "date": "2019-01-15 02:59:00 +0900", "snippet": "책 ‘함께 자라기’를 읽고 제목: 함께 자라기 - 애자일로 가는 길 지은이: 김창준 출판사: 인사이트자라기 경력과 전문성은 그다지 상관관계가 없다. 일을 오래했다고 해서 (연차가 높다고 해서) 실력이 늘지 않는다. 학습을 통한 성장을 이루어야 한다. 학습 프레임으로 사고하라. 일이나 삶을 성과나 성취 위주로 (실행 프레임) 사고하면 성과나 경쟁에 집중하여 학습하지 못함 현재 나에게 주어진 과업을 ‘내가 얼마나 배우느냐’로 바라볼 것 ..." }, { "title": "type erasure", "url": "/posts/type-erasure/", "categories": "dev, java", "tags": "java, generic", "date": "2018-12-20 02:59:00 +0900", "snippet": "참고: https://www.baeldung.com/java-type-erasure http://toby.epril.com/?p=248 http://wonwoo.ml/index.php/post/1743Type erasure can be explained as the process of enforcing type constraints only at compile time and discarding the element type information at runtime.타입 제한을 컴파일 타임에만 강제하고, 런타임에는 타입 ..." }, { "title": "r tips", "url": "/posts/r-tips/", "categories": "dev, r", "tags": "r, rstudio, setting", "date": "2018-11-29 02:59:00 +0900", "snippet": "ggmap 설치 시 tipsggmap 설치에 애먹으시는 분들을 위해서 몇가지 정리합니다.ggmap이 우리를 힘들게 하는 이유1) ggmap은 Google 지도 API를 쓰는데, Google이 과금을 시작했다.2) 그래서 Google API에 가입을 하고 키를 발급 받아야 한다. 그럼 돈이 드나? 그건 아니다. Google API에 인증 Key를 만들고 빌링 정보 등록안하면 ~무료 상태이고, 사용량의 제한을 받는다. 좋아!~ 안된다. 그대신 처음 무료 평가판으로 쓰고, 무료 사용량을 다 쓰고 나면 동의 없이 추가 과금을 하지는 ..." }, { "title": "python tips", "url": "/posts/python-tips/", "categories": "dev, python", "tags": "python", "date": "2018-11-24 02:59:00 +0900", "snippet": "3rd party library를 패키징하여 사용하기$ pip install -r requirements.txt -t ./libs$ cd ./libs &amp;amp;&amp;amp; zip -r libs.zip .이렇게 만든 libs.zip 파일을 python 코드에서import syssys.path.insert(0, &#39;libs.zip&#39;)하면 모듈들을 import 해서 사용할 수 있다.이 방식을 사용하면 python 프로젝트를 uber-jar 처럼 패키징해서 편하게 배포할 수 있다.예를 들어 PySpark 잡으..." }, { "title": "spark tips", "url": "/posts/spark-tips/", "categories": "dev, spark", "tags": "spark", "date": "2018-11-22 01:34:00 +0900", "snippet": "Spark UnitTest 공통 모듈 만들기 만들려는 것 로컬에서 하둡 클러스터 연결 없이 Spark core, Spark sql 모두 테스트 하기 File은 물론, Hive Table로 부터의 DataFrame 로드도 모두 로컬에서 가능하게 하기 Hive Table을 Sampling해서 파일로 Export해서 Project의 리소스로 추가 RDD나 DataFrame, DataSet에 대한 비교로 Assert 체크 하기 ..." }, { "title": "pyenv", "url": "/posts/pyenv/", "categories": "dev, python", "tags": "python, pyenv, setting", "date": "2018-11-20 02:59:00 +0900", "snippet": "pyenvhttps://github.com/pyenv/pyenv python version 및 virtual environment를 관리하기 위해서 사용 폴더별로 어떤 환경을 사용할 것인지 설정하면 자동으로 activate 되기 때문에 매우 편리Install pyenv$ brew install pyenv$ brew install pyenv-virtualenv$ brew install pyenv-which-ext$ vim ~/.zshrc 또는 vim ~/.bash_profile# pyenveval &quot;$(pyenv i..." }, { "title": "aho-corasick algorithm", "url": "/posts/aho-corasick/", "categories": "dev, nlp", "tags": "algorithm, nlp", "date": "2018-11-16 11:00:00 +0900", "snippet": "Aho-Corasick (아호-코라식) 참고 http://m.blog.naver.com/kks227/220992598966 https://www.slideshare.net/ssuser81b91b/ahocorasick-algorithm 아호 코라식 알고리즘(Aho–Corasick string matching algorithm)은 Alfred V. Aho와 Margaret J. Corasick이 고안한 패턴 집합에 대한 매칭 알고리즘이다.O(m + n + k)의 시간 복잡도로 패턴 집합에 대하여..." }, { "title": "효율적인 트랜스포메이션 (하이 퍼포먼스 스파크 chap5)", "url": "/spark/highperformancespark-chap5/", "categories": "dev, spark", "tags": "spark", "date": "2018-11-09 23:59:00 +0900", "snippet": "좁은 트랜스포메이션 vs. 넓은 트랜스포메이션좁은 종속성의 트랜스포메이션 부모 RDD의 각 파티션이 자식 RDD의 최대 하나의 파티션에 의해 사용되는 것 즉, 부모 파티션의 자식은 오직 하나 값을 모르고도 할 수 있는 것들 변환하거나: map, filter, mapPartition, … 합하거나: coalesce (셔플 없이) 넓은 종속성의 트랜스포메이션 부모의 각 파티션마다 여러 자식 파티션에 종속되어 있는 것 즉, 부모 파티션의 자식이 여럿. 셔플을 수반한: gro..." }, { "title": "조인(SQL과 코어 스파크) (하이 퍼포먼스 스파크 chap4)", "url": "/spark/highperformancespark-chap4/", "categories": "dev, spark", "tags": "spark", "date": "2018-11-07 23:59:00 +0900", "snippet": "코어 스파크 조인 일반적인 조인은 로컬에서 작업을 수행할 수 있도록 각 RDD에 연관되는 키가 같은 파티션에 있기를 요구하므로 비용이 비싸다. 조인의 비용은 키의 개수와 레코드가 올바른 파티션에 위치하기 위해 움직여야 하는 규모에 비례해서 커진다.조인 형태 선택하기 키 공간(개수)를 줄이기 위해 distinct나 combineByKey 연산을 수행하거나 중복키를 관리할 수 있는 cogroup을 사용하는 것이 나을 수 있다. 조인하기 전에 조인할 키의 개수를 줄여놓는 것이 (중복없이) 낫다는 이야기 ..." }, { "title": "information entropy", "url": "/posts/entropy/", "categories": "machine learning", "tags": "information theory, entropy, machine learning", "date": "2018-10-30 19:30:00 +0900", "snippet": "Information Entropy (Shannon Entropy)사건 X의 불확실성 또는 정보량의 기대 값 사건 X는 K가지 경우의 수를 가진 이산 변수. 예를들어 6가지 경우가 나오는 주사위 던지기모든 사건의 기대값 = Σ 확률변수 * 확률 주사위 던지기의 기대값 = 11/6 + 21/6 + 31/6 + 41/6 + 51/6 + 61/6 = 3.5모든 사건 정보량의 기대값 H(P) = -Σ log(P(x)) * P(x)각 사건의 발생 확률이 고르다면, 이 사건은 불확실성이 높다. 주사위 1~6 눈이 나올 확률이 고르..." }, { "title": "java off heap", "url": "/posts/java-off-heap/", "categories": "dev, java", "tags": "java, jvm, off-heap, unsafe", "date": "2018-10-26 23:59:00 +0900", "snippet": "Off heap memory allocation sun.misc.Unsafe: JVM internal API로서 생성자 호출없이 Object를 생성한다거나 하는 다양한 기능의 API가 있으나 대표적으로 Non-JVM 영역(off-heap, direct memory)에 메모리를 할당하여 사용할 수 있음 JVM 영역 밖의 메모리를 사용 ==&amp;gt; GC Overhead가 없음. 대신 leak이 나지 않게 개발자가 메모리 관리를 잘 해야 함 ..." }, { "title": "spark rdd의 coalesce와 partition의 차이", "url": "/posts/coalesce-and-repartition/", "categories": "dev, spark", "tags": "spark", "date": "2018-10-25 23:55:00 +0900", "snippet": "coalesce full shuffle을 하지 않고 파티션의 개수를 조절한다. transform 연산 후에 결과를 1개의 파일로 저장하기 위해 coalesce(1)를 하면, transform을 1개의 executor로 실행한다. 왜 그런지 추측해보면… coalesce 작업은 full shuffle을 하지 않으므로 DAG Schedule상 stage 분할이 되지 않는다. stage 분할 없이 partition을 1개로 합하려면 애초에 executor 하나로 partition을 몰아 넣고 시작..." }, { "title": "DataFrame, Dataset과 스파크 SQL (하이 퍼포먼스 스파크 chap3)", "url": "/spark/highperformancespark-chap3/", "categories": "dev, spark", "tags": "spark", "date": "2018-10-24 23:15:00 +0900", "snippet": "DataFrame and DatasetHistory DataFrame since Spark 1.3 Dataset since Spark 1.6 (preview) Unified since Spark 2.0 (type DataFrame = Dataset[Row]) Scala Type Alias: https://alvinalexander.com/scala/scala-type-aliases-syntax-examples DataFrames and Datasets are built on top of RDD..." }, { "title": "CDH5에서 Spark 2.x 돌리기", "url": "/posts/run-spark2.x-cdh5/", "categories": "dev, spark", "tags": "cdh5, spark", "date": "2018-10-23 17:20:00 +0900", "snippet": "사전 준비spark submit 목적의 gateway (개발 PC)에 설치를 진행hadoop client가 이미 설치되어 hadoop cluster와 잘 연동되고 있다고 가정 $HADOOP_HOME $HADOOP_CONF_DIR: hadoop cluster의 폴더 구조와 동일하게 $HIVE_HOME $HIVE_CONF_DIR: hadoop cluster의 폴더 구조와 동일하게Spark 설정은 yarn-cluster mode 기준Spark Client 설치yarn cluster가 java7을 사용하면 spark 2.1.X..." }, { "title": "spark tungsten", "url": "/posts/spark-tungsten/", "categories": "dev, spark", "tags": "spark, tungsten", "date": "2018-10-21 23:59:00 +0900", "snippet": "spark tungstenTungsten is the codename for the umbrella project to make changes to Apache Spark’s execution engine that focuses on substantially improving the efficiency of memory and CPU for Spark applications, to push performance closer to the limits of modern hardware. This effort includes the..." }, { "title": "spark dataframe을 csv로 export하기", "url": "/spark/spark-dataframe-export/", "categories": "dev, spark", "tags": "spark", "date": "2018-10-19 16:50:00 +0900", "snippet": "spark dataframe을 csv로 export하기jupyter, sparkmagic kernel 기준%%configure -f{&quot;jars&quot;: [&quot;/user/olaf.kido/spark-csv_2.10-1.5.0.jar&quot;, &quot;/user/olaf.kido/commons-csv-1.6.jar&quot;]}# Spark 1.6이라면sqlContext = HiveContext(sc)df = sqlContext \\.sql(&quot;select * from source_table&quot..." }, { "title": "jupyter notebook tips", "url": "/posts/jupyter-notebook-tips/", "categories": "dev, jupyter", "tags": "jupyter, notebook", "date": "2018-10-19 00:18:00 +0900", "snippet": "SparkMagicspark add jars%%configure -f{&quot;jars&quot;: [&quot;/user/olaf.kido/spark-csv_2.10-1.5.0.jar&quot;, &quot;/user/olaf.kido/commons-csv-1.6.jar&quot;]}첫번째 cell에서 %%configure 명령을 보내면 새로운 Spark Session이 시작됨.jar 파일은 hdfs의 path이다.자주쓰는 jar들은 livy server의 $LIVY_HOME/rsc_jars 폴더에 복사해두어도 됨. (확인..." }, { "title": "스파크는 어떻게 동작하는가? (하이 퍼포먼스 스파크 chap2)", "url": "/spark/highperformancespark-chap2/", "categories": "dev, spark", "tags": "spark", "date": "2018-10-17 13:00:00 +0900", "snippet": "RDD In Memory Immutable Dependency Narrow Wide Properties Partitions blocks, splits Parent dependencies Function (File, Pair, Shuffled, …) Partitioner..." }, { "title": "installing beakerx", "url": "/posts/installing-beakerx/", "categories": "dev, jupyter", "tags": "jupyter, beakerx, setting", "date": "2018-10-17 01:00:00 +0900", "snippet": "ipywidgets, QGrid, BeakerX 설치 ipywidgets: notebook에서 slider 같이 user input 받을 수 있는 widget QGrid: interactive한 grid view BeakerX: interactive한 grid view$ conda config --add channels conda-forge$ conda install ipywidgets$ conda install qgrid$ conda install nodejs$ jupyter labextension install @ju..." }, { "title": "installing jupyter lab on the ubuntu 16.04", "url": "/posts/installing-jupyter-lab/", "categories": "dev, jupyter", "tags": "jupyter, setting", "date": "2018-10-16 20:10:00 +0900", "snippet": "Install Anacondahttps://www.anaconda.com/download/#linux$ wget https://repo.anaconda.com/archive/Anaconda3-5.3.0-Linux-x86_64.sh$ ./Anaconda3-5.3.0-Linux-x86_64.sh$ source ~/.bashrcCreate a virtual environment$ conda create -n jupyter$ vim ~/.bashrcconda activate jupyter$ source ~/.bashrcInstall ..." }, { "title": "nginx + uwsgi + flask로 rest api 개발 시 post method의 request body 유실 케이스", "url": "/posts/uwsgi-lost-req-body/", "categories": "dev, flask", "tags": "flask, uwsgi", "date": "2018-06-01 23:19:00 +0900", "snippet": "이슈nginx + uwsgi + flask로 rest api 서버를 구성. spring webflux로 만든 다른 api 서버에서 flask로 post 요청을 보내면 flask app에서는 request body가 보이지 않음. nginx access로그에서는 보임. 특이한 것은 다른 client (예를 들어 postman)에서 같은 명령을 보내면 잘 됨. 반대로 client는 동일한데 flask를 flask run (즉, werkzeug)로 띄우면 이것도 잘 됨.원인현재까지 파악한 원인은 문제가 되는 경우에 http head..." }, { "title": "좋은 TEST CASE를 만드는 법", "url": "/posts/checklist-for-test-case/", "categories": "dev, retrospective", "tags": "test case, acceptance criteria", "date": "2015-09-18 23:00:00 +0900", "snippet": "이번 스프린트에서는 Test Case가 치밀하지 못한 덕에 테스트 및 버그 수정 기간이 예상보다 많이 늘어졌음 Acceptance Criteria를 100% 반영할 것 Exception Handling을 검증할 것 성능이 특히 중요한 경우, Benchmark할 수 있는 Test Case를 추가할 것 기능적으로 문제가 없으나, 서비스에 반영하면 성능이 나오지 않아 이슈가 되는 것을 사전에 검증 Acceptance Criteria에 성능 목표가 있다면 더욱 훌륭함 " }, { "title": "APACHE FLUME VS. LOGSTASH", "url": "/posts/flume-logstash/", "categories": "dev, logstash", "tags": "hadoop, flume, logstash", "date": "2015-06-20 23:00:00 +0900", "snippet": "로그를 Tailing해서 Kafka로 전송하기 위해서 Apache Flume과 Logstash를 검토하다가 Logstash로 결정.일단 요구사항은 다음과 같았다. Source 로그 파일을 Tailing 해야 함. (즉, 계속 Write하고 있는 파일을 읽어들여야 함) 특정 디렉토리 안에 있는 복수 개의 로그 파일을 읽어야 함. 수집기가 의도적 혹은 장애로 중지되어서 재기동 하는 경우에 파일을 마지막에 읽었던 지점부터 읽기 시작해야 함. Target 로그를 Kafk..." }, { "title": "데이터 적재 체크 리스트", "url": "/posts/checklist-for-data-import/", "categories": "dev, data engineering", "tags": "hadoop, etl, hbase, nosql", "date": "2015-06-10 23:30:00 +0900", "snippet": "데이터 엔지니어링의 시작은 데이터를 저장소에 저장하는 것으로 부터 출발한다.일회성으로 데이터를 적재하는 것이 아니라 지속적으로 생성되는 데이터를 저장 관리해야 한다면 어떤 것들을 먼저 고려해야 할까?데이터 활용 목적 무엇을 분석하기 위한 데이터인가에 따라 데이터 저장 형태(스키마), 저장 시스템, 관리 체계 등이 달라질 수 있다. 가장 먼저 해야하는 것은 무엇을 위한 데이터인지 파악하는 것이다.데이터 속성 파악 데이터 스키마 데이터 사이즈 예) 최초 적재 데이터량, 일일 증가 예측량 원천 데..." }, { "title": "HIVE QUERY의 OUTPUT이 작은 파일로 쪼개지는 경우", "url": "/posts/hive-merge-output/", "categories": "dev, hive", "tags": "hadoop, hive", "date": "2015-06-10 23:00:00 +0900", "snippet": "MapReduce 처리에 있어서 Input 파일들이 Block 크기 이하로 잘게 쪼개져 있는 경우는 좋지 않다.MapReduce Job에서 Map Task의 개수는 Input의 Block 개수에 의해 결정이 되는데, Block 크기 이하의 파일들이 많으면 그만큼 많은 Mapper가 생성되고, 각 Mapper는 작은량의 데이터만 처리하게 되기 때문이다.Hive에서 Query를 수행하는 경우 Output 파일의 개수가 많고 각 파일의 크기가 Block 크기 이하로 작게 생성되는 경우가 있다. 이는 Hive가 Reduce Task의..." }, { "title": "FLUME KAFKASINK에서 KAFKA LOG의 PARTITIONING KEY 설정", "url": "/posts/flume-kafkasink/", "categories": "dev, flume", "tags": "hadoop, flume", "date": "2015-05-28 23:00:00 +0900", "snippet": "Flume KafkaSink로 메시지를 Kafka에 쌓아 보았더니, 하나의 Partition에만 로그를 쌓다가 10분 간격으로 Partition이 변경되었다.이렇게 된 이유는 Kafka에 로그를 쌓을 때 (KeyedMessage) Key를 명시하지 않으면 Kafka가 아래와 같이 Partitioning을 수행하기 때문이다. when the partitioning key is not specified or null, a producer will pick a random partition and stick to it for so..." }, { "title": "오픈소스 분석 체크 리스트", "url": "/posts/oss-checklist/", "categories": "dev, retrospective", "tags": "open source, oss", "date": "2015-05-26 23:00:00 +0900", "snippet": " 왜? 어떤 문제를 해결하기 위한 기술인가? 기존 기술의 어떤 문제를 해결하기 위해 나왔는가? 어떻게?*문제를 어떻게 해결하는가? 기존에 유사한 시도는 없었는가? 기존의 유사한 시도가 가지고 있던 한계점은 어떤 것들이 있었나? 신기술에서는 이를 어떻게 해결하고 있나? 아키텍처 성능 성능 결정 요인은? 예상 병목 구간은? ..." }, { "title": "FLUME PERFORMANCE TUNING", "url": "/posts/flume-performance-tuning/", "categories": "dev, flume", "tags": "hadoop, flume", "date": "2015-05-22 23:00:00 +0900", "snippet": "Batch Size (Source/Sink) How-to: Do Apache Flume Performance Tuning (Part 1) 하나의 트랜잭션에 Source가 Channel에 Put하거나 Sink가 Take하는 Event 개수 만일 batch size가 적은 개수만큼 이벤트가 발생하면? batch timeout안에 batch size가 다 차지 않으면 Put/Take 수행함. (무한정 기다리지 않음) batch timeo..." }, { "title": "구글은 어떻게 일하는가 (에릭 슈미트, 조너선 로젠버그 지음, 김영사)", "url": "/posts/how-google-works/", "categories": "book", "tags": "구글은 어떻게 일하는가, 구글", "date": "2015-02-28 23:00:00 +0900", "snippet": "구글은 Smart &amp;amp; Creative로 표현되는 인재들을 모아 마음껏 일하고 능력대로 성과를 내도록 하는 곳인 것 같다. 역시 IT에서 중요한 것은 기술력과 상상력! 둘을 엮어 혁신적 제품으로 만들 수 있는 기업 문화/조직을 갖고 있느냐가 핵심. 책의 핵심 내용이 잘 요약된 에릭 슈미트의 발표자료 http://www.slideshare.net/alleciel/how-google-works-korean " }, { "title": "작업 시간 추정의 습관화", "url": "/posts/estimating-tasks/", "categories": "dev, retrospective", "tags": "agile, task, estimation, 추정", "date": "2014-10-28 23:00:00 +0900", "snippet": "어떤 일을 시작하든지 간에 맨 처음 해야 하는 일은 ‘계획’을 세우는 일이다. 혼자하는 일이라면 상관 없겠지만, 회사에서 하는 일이라면 계획 수립은 필수다. 보통 계획이라고 하면 크게 두 가지 내용을 담고 있다. 하나는 해야 할 일들의 목록이고 나머지 하나는 일정이다. 간단히 말해 계획이란 어떤 일들을 언제까지 완료해야 한다는 것이다. 이 중에 특히 더 어렵게 느껴지는 부분은 일정 수립이다. 왜냐하면 일정 수립은 순전히 ‘가설’이기 때문이다.의도적 수련 항목에 작업 계획 세우기를 추가한다. 파트 리더 할 때부터 유난히 WBS ..." }, { "title": "여덟단어 (박웅현 지음, 북하우스)", "url": "/posts/eight-words/", "categories": "book", "tags": "여덟단어, 박웅현", "date": "2014-07-21 23:00:00 +0900", "snippet": "1. 자존“자존감을 가지는 데 가장 방해가 되는 요인은 아마 우리 교육이 아닐까 싶습니다. 우리나라 교육은 아이들 각자가 가지고 있는 것에 기준을 두고 그것을 끄집어 내기보다 기준점을 바깥에 찍죠.” (p20)“미국 교육은 ‘네 안에 있는 것은 무엇인가’를 궁금해 한다면 한국 교육은 ‘네 안에 무엇을 넣어야 할 것인가’를 고민하는 것이 가장 큰 차이” (p26)“자신의 길을 무시하지 않는 것, 바로 이게 인생입니다. 그리고 모든 인생마다 기회는 달라요. 왜냐하면 내가 어디에 태어날지, 어떤 환경에서 자랄지 아무도 모르잖아요?..." } ]
